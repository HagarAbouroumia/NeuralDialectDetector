adam_epsilon: 1.0e-08
adapter_type: plain_adapter
batch_size: 32
bottleneck_dim: 384
checkpoint_on_improvement: true
checkpointing_freq: 1
checkpointing_on: false
checkpointing_path: checkpoints_marbert
class_index: -1
classif_dropout_rate: 0.3
current_adapter_to_train: 6
device: cuda
early_stopping: true
early_stopping_patience: 10
handle_imbalance_sampler: false
improvement_check_freq: 100
indexes_filtration_path: null
initial_learning_rate: 5.0e-06
inv-sqrt-lr-max-factor: 10
inv-sqrt-lr-min-factor: 0.01
inv-sqrt-lr-temperature: 1
invsqrt-lr-scheduler: false
is_MSA: true
is_province: true
lr-mini-epoch-size: 10
masking_percentage: 0
max_ex_per_class: null
max_sequence_length: 110
model_class: ArabicDialectBERT
model_name_path: UBC-NLP/MARBERT
neptune_experiment_name: MARBERT_MSA_Province_110_FT_2
neptuneaiAPI: eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiYzNlOTI4MDUtMWEwMC00N2U3LWEzYTAtZjAyMzA2YzE1YjI3In0=
no_total_adapters: 21
num_epochs: 10
one_class_filtration: null
path_to_data: NADI2021_DEV.1.0/NADI2021_DEV.1.0/Subtask_1.1+2.1_MSA
run_title: MARBERT_MSA_Province_110_FT_2
save_final_model: false
seed_list:
- 1234
- 611
- 520
- 300
- 760
- 42
- 1111
- 1234
- 10
- 1
- 20
- 5
- 2
- 100
- 512
stage_2_training: true
use_adapt_after_fusion: true
use_adapters: false
use_neptune: true
use_regional_mapping: false
use_vert_att: false
vatt-bottleneck_dim: 384
vatt-final-adapter: false
vatt-positional-keys: random
vatt-use-common-transform: false
warmup_steps: 250
